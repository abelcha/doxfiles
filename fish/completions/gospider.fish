complete -c gospider -s s -l site -d 'string               Site to crawl'
complete -c gospider -s S -l sites -d 'string              Site list to crawl'
complete -c gospider -s p -l proxy -d 'string              Proxy (Ex: http://127.0.0.1:8080)'
complete -c gospider -s o -l output -d 'string             Output folder'
complete -c gospider -s u -l user-agent -d 'string         User Agent to use'
complete -c gospider -l cookie -d 'string             Cookie to use (testA=a; testB=b)'
complete -c gospider -s H -l header -d 'stringArray        Header to use (Use multiple flag to set multiple header)'
complete -c gospider -l burp -d 'string               Load headers and cookie from burp raw http request'
complete -c gospider -l blacklist -d 'string          Blacklist URL Regex'
complete -c gospider -l whitelist -d 'string          Whitelist URL Regex'
complete -c gospider -l whitelist-domain -d 'string   Whitelist Domain'
complete -c gospider -s L -l filter-length -d 'string      Turn on length filter'
complete -c gospider -s t -l threads -d 'int               Number of threads (Run sites in parallel) (default 1)'
complete -c gospider -s c -l concurrent -d 'int            The number of the maximum allowed concurrent requests of the matching domains (default 5)'
complete -c gospider -s d -l depth -d 'int                 MaxDepth limits the recursion depth of visited URLs. (Set it to 0 for infinite recursion) (default 1)'
complete -c gospider -s k -l delay -d 'int                 Delay is the duration to wait before creating a new request to the matching domains (second)'
complete -c gospider -s K -l random-delay -d 'int          RandomDelay is the extra randomized duration to wait added to Delay before creating a new request (second)'
complete -c gospider -s m -l timeout -d 'int               Request timeout (second) (default 10)'
complete -c gospider -s B -l base -d 'Disable all and only use HTML content'
complete -c gospider -l js -d 'Enable linkfinder in javascript file (default true)'
complete -c gospider -l sitemap -d 'Try to crawl sitemap.xml'
complete -c gospider -l robots -d 'Try to crawl robots.txt (default true)'
complete -c gospider -s a -l other-source -d 'Find URLs from 3rd party (Archive.org, CommonCrawl.org, VirusTotal.com, AlienVault.com)'
complete -c gospider -s w -l include-subs -d 'Include subdomains crawled from 3rd party. Default is main domain'
complete -c gospider -s r -l include-other-source -d "Also include other-source's urls (still crawl and request)"
complete -c gospider -l subs -d 'Include subdomains'
complete -c gospider -l debug -d 'Turn on debug mode'
complete -c gospider -l json -d 'Enable JSON output'
complete -c gospider -s v -l verbose -d 'Turn on verbose'
complete -c gospider -s q -l quiet -d 'Suppress all the output and only show URL'
complete -c gospider -l no-redirect -d 'Disable redirect'
complete -c gospider -n __fish_no_arguments -l version -d 'Check version'
complete -c gospider -s l -l length -d 'Turn on length'
complete -c gospider -s R -l raw -d 'Enable raw output'
complete -c gospider -s h -l help -d 'help for gospider'
